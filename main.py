import json
from typing import List, Optional, Dict
import nltk
import re
import pymorphy3 as pym
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from fuzzywuzzy import fuzz

import uvicorn
from fastapi import FastAPI, Request, Depends
from pydantic import BaseModel
from contextlib import asynccontextmanager

import ssl

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL (–Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω–æ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

class LawLink(BaseModel):
    law_id: Optional[int] = None
    article: Optional[str] = None
    point_article: Optional[str] = None
    subpoint_article: Optional[str] = None


class LinksResponse(BaseModel):
    links: List[LawLink]


class TextRequest(BaseModel):
    text: str


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
morph_analyzer = None
stopwords_ru = None
law_aliases_invers = None


def normalize_text(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    global morph_analyzer, stopwords_ru
    
    tokens = word_tokenize(text.lower(), language='russian')
    normalized_tokens = []
    
    for token in tokens:
        if re.match(r'^\d+\.\d+', token) or re.match(r'^\d+(?:\.\d+)+$', token):
            normalized_tokens.append(token)
        elif token.isdigit() or (any(c.isdigit() for c in token) and any(c.isalpha() for c in token)):
            normalized_tokens.append(token)
        elif token in ['—Å—Ç', '–ø', '–ø–ø', '—Å—Ç.', '–ø.', '–ø–ø.', '–Ω–∫', '–≥–∫', '—É–∫', '—Ç–∫', '–∞–ø–∫', '–±–∫', '–∫–æ–∞–ø', '—Ä—Ñ', '—á']:
            normalized_tokens.append(token.split('.')[0])
        elif token.isalpha() and token not in stopwords_ru:
            parsed = morph_analyzer.parse(token)[0]
            normalized_tokens.append(parsed.normal_form)
        elif token != '.':
            normalized_tokens.append(token)

    return ' '.join(normalized_tokens)


def extract_multiple_entities(raw: str):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
    clean_subpoint = re.sub(r'[^\d–∞-—è,\s\.\-]', '', raw.lower())
    clean_subpoint = re.sub(r'(?<!\d)\.(?!\d)', '', clean_subpoint)
    parts = re.split(r'[,–∏]', clean_subpoint)
    
    entities = []
    for part in parts:
        part = part.strip()
        if part:
            entities.append(part)
    
    return entities if entities else [raw]


def find_law_id_fuzzy(law_name):
    """–ü–æ–∏—Å–∫ ID –∑–∞–∫–æ–Ω–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Å –ø–æ–º–æ—â—å—é –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    global law_aliases_invers
    
    max_score = 0
    for law_key in law_aliases_invers.keys():
        lev_score = fuzz.partial_ratio(law_name, law_key.lower())
        if lev_score > max_score:
            max_score = lev_score
            res_key = law_key
    if max_score < 90:
        return None
    else:
        return law_aliases_invers[res_key]


def process_match(match, context):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ LawLink"""
    references = []
    groups = match.groups()

    articles = []
    point_articles = []
    subpoint_articles = []

    if match.group('–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞') is None:
        point_articles = [None]
        if match.group('—á–∞—Å—Ç—å_–Ω–æ–º–µ—Ä–∞') is not None:
            if ',' in match.group('—á–∞—Å—Ç—å_–Ω–æ–º–µ—Ä–∞') or '–∏' in match.group('—á–∞—Å—Ç—å_–Ω–æ–º–µ—Ä–∞'):
                point_articles = extract_multiple_entities(match.group('—á–∞—Å—Ç—å_–Ω–æ–º–µ—Ä–∞'))
            else:
                point_articles = [match.group('—á–∞—Å—Ç—å_–Ω–æ–º–µ—Ä–∞')]
        else:
            point_articles = [None]
    else:
        if ',' in match.group('–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞') or '–∏' in match.group('–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞'):
            point_articles = extract_multiple_entities(match.group('–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞'))
        else:
            point_articles = [match.group('–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞')]

    if match.group('–ø–æ–¥–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞') is None:
        subpoint_articles = [None]
    else:
        if ',' in match.group('–ø–æ–¥–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞') or '–∏' in match.group('–ø–æ–¥–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞'):
            subpoint_articles = extract_multiple_entities(match.group('–ø–æ–¥–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞'))
        else:
            subpoint_articles = [match.group('–ø–æ–¥–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞')]

    if match.group('—Å—Ç–∞—Ç—å—è_–Ω–æ–º–µ—Ä–∞') is None:
        articles = [None]
    else:
        if ',' in match.group('—Å—Ç–∞—Ç—å—è_–Ω–æ–º–µ—Ä–∞') or '–∏' in match.group('—Å—Ç–∞—Ç—å—è_–Ω–æ–º–µ—Ä–∞'):
            articles = extract_multiple_entities(match.group('—Å—Ç–∞—Ç—å—è_–Ω–æ–º–µ—Ä–∞'))
        else:
            articles = [match.group('—Å—Ç–∞—Ç—å—è_–Ω–æ–º–µ—Ä–∞')]

    law_id = find_law_id_fuzzy(match.group('–æ—Å—Ç–∞–ª—å–Ω–æ–µ'))

    for article in articles:
        for point_article in point_articles:
            for subpoint_article in subpoint_articles:
                reference = LawLink(
                    law_id=law_id,
                    article=article,
                    point_article=point_article,
                    subpoint_article=subpoint_article
                )
                references.append(reference)

    return references


def find_references_in_text(original_text: str) -> List[LawLink]:
    """–ü–æ–∏—Å–∫ –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π"""
    references = []

    patterns = [
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Å—ã–ª–∫–∏
        r'(?:–≤\s+)?(?:(?P<–ø–æ–¥–ø—É–Ω–∫—Ç_–∫–ª—é—á>–ø–ø\.|–ø–æ–¥–ø—É–Ω–∫—Ç[–∞-—è]{0,7}|–ø–æ–¥–ø\.)\s*(?P<–ø–æ–¥–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞>(?:\d{1,4}[–∞-—è]?|[–∞-—è])(?:\s*,\s*(?:\d{1,4}[–∞-—è]?|[–∞-—è]))*(?:\s*–∏\s*(?:\d{1,4}[–∞-—è]?|[–∞-—è]))?)\s+)?'
        r'(?:–≤\s+)?(?:(?P<–ø—É–Ω–∫—Ç_–∫–ª—é—á>–ø\.|–ø—É–Ω–∫—Ç[–∞-—è]{0,5}|–ø—É–Ω—Ç[–∞-—è]{0,5})\s*(?P<–ø—É–Ω–∫—Ç_–Ω–æ–º–µ—Ä–∞>(?:\d{1,4}(?:[\.\-]\d{1,3})*[–∞-—è]?|[–∞-—è])(?:\s*,\s*(?:\d{1,4}(?:[\.\-]\d{1,3})*[–∞-—è]?|[–∞-—è]))*(?:\s*–∏\s*(?:\d{1,4}(?:[\.\-]\d{1,3})*[–∞-—è]?|[–∞-—è]))?)\s+)?'
        r'(?:–≤\s+)?(?:(?P<—á–∞—Å—Ç—å_–∫–ª—é—á>—á\.|—á–∞—Å—Ç[—å–∏])\s*(?P<—á–∞—Å—Ç—å_–Ω–æ–º–µ—Ä–∞>(?:\d{1,3}(?:\.\d{1,3})?|[–∞-—è])(?:\s*,\s*(?:\d{1,3}(?:\.\d{1,3})?|[–∞-—è]))*(?:\s*–∏\s*(?:\d{1,3}(?:\.\d{1,3})?|[–∞-—è]))?)(?:\s*,\s*)?\s+)?'
        r'(?:–≤\s+)?(?:(?P<—Å—Ç–∞—Ç—å—è_–∫–ª—é—á>—Å—Ç\.|—Å—Ç–∞—Ç—å[–µ–π–∏—é—è]|—Å—Ç–∞—Ç–µ–π?|—Å—Ç–∞—Ç—å—è)\s*(?:(?:–≤|–Ω–∞|–ø–æ)\s+)?(?P<—Å—Ç–∞—Ç—å—è_–Ω–æ–º–µ—Ä–∞>(?:\d{1,4}(?:[\.\-]\d{1,3})*[–∞-—è]?)(?:\s*,\s*(?:\d{1,4}(?:[\.\-]\d{1,3})*[–∞-—è]?))*(?:\s*–∏\s*(?:\d{1,4}(?:[\.\-]\d{1,3})*[–∞-—è]?))?)\s+)?'
        r'(?P<–æ—Å—Ç–∞–ª—å–Ω–æ–µ>(?:'
        # –ö–æ–¥–µ–∫—Å—ã
        r'(?:–ê—Ä–±–∏—Ç—Ä–∞–∂–Ω(?:–æ–≥–æ|—ã–π)\s+–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω(?:–æ–≥–æ|—ã–π)|–ë—é–¥–∂–µ—Ç–Ω(?:–æ–≥–æ|—ã–π)|–í–æ–¥–Ω(?:–æ–≥–æ|—ã–π)|–í–æ–∑–¥—É—à–Ω(?:–æ–≥–æ|—ã–π)|–ì—Ä–∞–¥–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω(?:–æ–≥–æ|—ã–π)|–ì—Ä–∞–∂–¥–∞–Ω—Å–∫(?:–æ–≥–æ|–∏–π)|–ì—Ä–∞–∂–¥–∞–Ω—Å–∫(?:–æ–≥–æ|–∏–π)\s+–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω(?:–æ–≥–æ|—ã–π)|–ñ–∏–ª–∏—â–Ω(?:–æ–≥–æ|—ã–π)|–°–µ–º–µ–π–Ω(?:–æ–≥–æ|—ã–π)|–¢–∞–º–æ–∂–µ–Ω–Ω(?:–æ–≥–æ|—ã–π)|–¢—Ä—É–¥–æ–≤(?:–æ–≥–æ|–æ–π)|–£–≥–æ–ª–æ–≤–Ω–æ-–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω(?:–æ–≥–æ|—ã–π)|–£–≥–æ–ª–æ–≤–Ω–æ-–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω(?:–æ–≥–æ|—ã–π)|–£–≥–æ–ª–æ–≤–Ω(?:–æ–≥–æ|—ã–π)|–õ–µ—Å–Ω(?:–æ–≥–æ|–æ–π)|–ù–∞–ª–æ–≥–æ–≤(?:–æ–≥–æ|—ã–π)|–ó–µ–º–µ–ª—å–Ω(?:–æ–≥–æ|—ã–π))\s+–∫–æ–¥–µ–∫—Å(?:–∞|)(?:\s+–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏|\s+–†–æ—Å—Å–∏–∏|\s+–†–§|)'
        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        r'|–ê–ü–ö(?:\s+(?:–†–æ—Å—Å–∏–∏|–†–§))?|–ë–ö(?:\s+(?:–†–æ—Å—Å–∏–∏|–†–§))?|–ì–ö(?:\s+–†–§)?|–ì–ü–ö(?:\s+(?:–†–æ—Å—Å–∏–∏|–†–§))?|–ñ–ö(?:\s+(?:–†–æ—Å—Å–∏–∏|–†–§))?|–°–ö(?:\s+(?:–†–æ—Å—Å–∏–∏|–†–§))?|–¢–ö(?:\s+–†–§)?'
        r'|–£–ò–ö(?:\s+(?:–†–æ—Å—Å–∏–∏|–†–§))?|–£–ü–ö(?:\s+(?:–†–æ—Å—Å–∏–∏|–†–§))?|–£–ö(?:\s+(?:–†–æ—Å—Å–∏–∏|–†–§))?|–õ–ö(?:\s+(?:–†–æ—Å—Å–∏–∏|–†–§))?|–ù–ö(?:\s+(?:–†–æ—Å—Å–∏–∏|–†–§))?|–ó–ö(?:\s+(?:–†–æ—Å—Å–∏–∏|–†–§))?'
        # –ö–æ–¥–µ–∫—Å—ã –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö
        r'|–ö–æ–¥–µ–∫—Å(?:–∞|)(?:\s+–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏|\s+–†–æ—Å—Å–∏–∏|\s+–†–§|)?\s+–æ–±\s+–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö\s+–ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö|–ö–æ–ê–ü(?:\s+–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏|\s+–†–æ—Å—Å–∏–∏|\s+–†–§|)?'
        # –î—Ä—É–≥–∏–µ –∫–æ–¥–µ–∫—Å—ã
        r'|–ö–æ–¥–µ–∫—Å(?:–∞|)\s+–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ\s+—Å—É–¥–æ–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞(?:\s+–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏|\s+–†–æ—Å—Å–∏–∏|\s+–†–§|)?'
        r'|–ö–æ–¥–µ–∫—Å(?:–∞|)\s+–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ\s+–≤–æ–¥–Ω–æ–≥–æ\s+—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞(?:\s+–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏|\s+–†–æ—Å—Å–∏–∏|\s+–†–§|)?'
        r'|–ö–æ–¥–µ–∫—Å(?:–∞|)\s+—Ç–æ—Ä–≥–æ–≤–æ–≥–æ\s+–º–æ—Ä–µ–ø–ª–∞–≤–∞–Ω–∏—è(?:\s+–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏|\s+–†–æ—Å—Å–∏–∏|\s+–†–§|)?'
        # –£–∫–∞–∑—ã –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞
        r'|–£–∫–∞–∑(?:–∞|)(?:\s+–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞(?:\s+–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏|\s+–†–æ—Å—Å–∏–∏|\s+–†–§|)?)?(?:\s+(?:‚Ññ?\s*\d+|\s*–æ—Ç\s*\d{2}\.\d{2}\.\d{4}))?(?:\s*¬´[^¬ª]*¬ª)?'
        # –†–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏—è –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞
        r'|–†–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏(?:—è|–µ)(?:\s+–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞(?:\s+–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏|\s+–†–æ—Å—Å–∏–∏|\s+–†–§|)?)?(?:\s+(?:‚Ññ?\s*\d+-\s*—Ä–ø|–æ—Ç\s*\d{2}\.\d{2}\.\d{4}))?(?:\s*¬´[^¬ª]*¬ª)?'
        r'|–†–ü(?:\s+(?:‚Ññ?\s*\d+-\s*—Ä–ø|–æ—Ç\s*\d{2}\.\d{2}\.\d{4}))?(?:\s*¬´[^¬ª]*¬ª)?'
        # –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ –∑–∞–∫–æ–Ω—ã
        r'|–§–µ–¥–µ—Ä–∞–ª—å–Ω(?:–æ–≥–æ|—ã–π)\s+–∑–∞–∫–æ–Ω(?:–∞|)(?:\s+[^,\.;]*)?'
        r'|–§–ó(?:\s+[^,\.;]*)?'
        r'|–ó–∞–∫–æ–Ω–∞\s+¬´[^¬ª]*¬ª'
        r'|–ó–∞–∫–æ–Ω–∞\s+"[^"]*"'
        # –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è
        r'|–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏(?:–∏|—è)(?:\s+–†–§|\s+–†–æ—Å—Å–∏–∏|\s+–†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏)?'
        # –û—Å–Ω–æ–≤—ã –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞
        r'|–û—Å–Ω–æ–≤—ã\s+–∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞(?:\s+(?:–†–æ—Å—Å–∏–π—Å–∫–æ–π\s+–§–µ–¥–µ—Ä–∞—Ü–∏–∏|–†–æ—Å—Å–∏–∏|–†–§))?(?:\s+‚Ññ?\s*\d+(?:-I)?)?(?:\s+–æ—Ç\s+\d{2}\.\d{2}\.\d{4})?(?:\s*¬´[^¬ª]*¬ª)?'
        # –ó–∞–∫–æ–Ω—ã –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏
        r'|–ó–∞–∫–æ–Ω(?:\s+(?:–†–æ—Å—Å–∏–π—Å–∫–æ–π\s+–§–µ–¥–µ—Ä–∞—Ü–∏–∏|–†–æ—Å—Å–∏–∏|–†–§))?(?:\s+‚Ññ?\s*\d+(?:-I)?)?(?:\s+–æ—Ç\s+\d{2}\.\d{2}\.\d{4})?(?:\s*¬´[^¬ª]*¬ª)?'
        # –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–≥–æ —É—á–µ—Ç–∞
        r'|–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π\s+—Å—Ç–∞–Ω–¥–∞—Ä—Ç\s+–±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–≥–æ\s+—É—á–µ—Ç–∞(?:\s+(?:–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö\s+—Ñ–∏–Ω–∞–Ω—Å–æ–≤|–¥–ª—è\s+–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π\s+–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ\s+—Å–µ–∫—Ç–æ—Ä–∞))?(?:\s+–§–°–ë–£\s*\d+(?:\/\d{4})?)?(?:\s*¬´[^¬ª]*¬ª)?'
        r'|–§–°–ë–£(?:\s+(?:–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö\s+—Ñ–∏–Ω–∞–Ω—Å–æ–≤|–¥–ª—è\s+–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π\s+–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ\s+—Å–µ–∫—Ç–æ—Ä–∞))?(?:\s*\d+(?:\/\d{4})?)?(?:\s*¬´[^¬ª]*¬ª)?'
        # –ü–æ–ª–æ–∂–µ–Ω–∏—è –ø–æ –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–º—É —É—á–µ—Ç—É
        r'|–ü–æ–ª–æ–∂–µ–Ω–∏–µ\s+–ø–æ\s+–±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–º—É\s+—É—á–µ—Ç—É(?:\s+–ü–ë–£\s*\d+(?:\/\d{4})?)?(?:\s*¬´[^¬ª]*¬ª)?'
        r'|–ü–ë–£(?:\s*\d+(?:\/\d{4})?)?(?:\s*¬´[^¬ª]*¬ª)?'
        # –ü–æ–ª–æ–∂–µ–Ω–∏–µ –ø–æ –≤–µ–¥–µ–Ω–∏—é –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–≥–æ —É—á–µ—Ç–∞
        r'|–ü–æ–ª–æ–∂–µ–Ω–∏(?:—è|–µ)\s+–ø–æ\s+–≤–µ–¥–µ–Ω–∏—é\s+–±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–≥–æ\s+—É—á–µ—Ç–∞\s+–∏\s+–±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–π\s+–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏\s+–≤\s+–†–æ—Å—Å–∏–π—Å–∫–æ–π\s+–§–µ–¥–µ—Ä–∞—Ü–∏–∏'
        r')'
        r'(?=\s|,|\.|;|$))'
    ]

    all_matches = []
    for pattern in patterns:
        matches = re.finditer(pattern, original_text, re.IGNORECASE)
        all_matches.append(matches)

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤—ã–≤–æ–¥–∏–º
    unique_matches = list(dict.fromkeys(all_matches))

    iter_count = 0
    for matches in unique_matches:
        for match in matches:
            iter_count += 1
            print(f"Match {iter_count}: {match.groups()}")
            references.extend(process_match(match, original_text))

    return references


def extract_legal_references_advanced(text: str):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    references = []
    sentence_references = find_references_in_text(text)
    references.extend(sentence_references)
    return references


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global morph_analyzer, stopwords_ru, law_aliases_invers
    
    print("üöÄ –°–µ—Ä–≤–∏—Å –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ NLTK
    print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ NLTK...")
    try:
        nltk.download('punkt')
        nltk.download('punkt_tab')
        nltk.download('stopwords')
    except ssl.SSLError as e:
        print(f"–û—à–∏–±–∫–∞ SSL: {e}")
    except Exception as e:
        print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã...")
    morph_analyzer = pym.MorphAnalyzer()
    stopwords_ru = stopwords.words("russian")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–ª–∏–∞—Å—ã –∑–∞–∫–æ–Ω–æ–≤
    print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–∞–∫–æ–Ω–æ–≤...")
    with open("law_aliases.json", "r", encoding='utf-8') as file:
        codex_aliases = json.load(file)
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
    law_aliases_invers = {i: k for k, v in codex_aliases.items() for i in v}
    
    app.state.codex_aliases = codex_aliases
    app.state.law_aliases_invers = law_aliases_invers
    app.state.morph_analyzer = morph_analyzer
    app.state.stopwords_ru = stopwords_ru
    
    print("‚úÖ –°–µ—Ä–≤–∏—Å –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    yield
    
    # Shutdown
    print("üõë –°–µ—Ä–≤–∏—Å –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è...")
    del codex_aliases
    del law_aliases_invers
    del morph_analyzer
    del stopwords_ru


def get_codex_aliases(request: Request) -> Dict:
    return request.app.state.codex_aliases


app = FastAPI(
    title="Law Links Service",
    description="C–µ—Ä–≤–∏—Å –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
    version="1.0.0",
    lifespan=lifespan
)


@app.post("/detect")
async def get_law_links(
    data: TextRequest,
    request: Request,
    codex_aliases: Dict = Depends(get_codex_aliases),
    ) -> LinksResponse:
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
    """
    print(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–æ–π {len(data.text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    try:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        references = extract_legal_references_advanced(data.text)
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(references)} –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫")
        
        return LinksResponse(links=references)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}")
        return LinksResponse(links=[])


@app.get("/health")
async def health_check():
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞
    """
    return {"status": "healthy"}



if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8978)